{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latest-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next todos:\n",
    "# construct a vector for one bounding box\n",
    "#    - fetch all the features within a bb (allready works) [x]\n",
    "#    - wheig the features according to their median size in sample dataset [x]\n",
    "#    - feed words acording to weiths into a document  [x]\n",
    "#    - generate a vector for this document [x]\n",
    "# need to clip feautres with the boundingbox [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "seventh-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import parse_osm\n",
    "import tag_handler\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compliant-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spatial_Semantic_Vector:\n",
    "    \n",
    "    def __init__(self, tag_df_path, tag_median_size_df_path):\n",
    "        self.tag_df =  pd.read_pickle(tag_df_path)\n",
    "        self.tag_median_size_df = pd.read_pickle(tag_median_size_df_path).set_index(['key','value']).sort_index()\n",
    "        # replace pd nan with None\n",
    "        self.tag_median_size_df = self.tag_median_size_df.where(pd.notnull(self.tag_median_size_df), None)\n",
    "        self.filtertags_handler = tag_handler.filtertags(self.tag_df)\n",
    "        # from here on out we want key value in the tag_df as the index because it makes\n",
    "        # the look up faster\n",
    "        # we sort the index to handle the \"1: PerformanceWarning: indexing past lexsort \n",
    "        # depth may impact performance.\" Warning\n",
    "        self.tag_df = self.tag_df.set_index(['key','value']).sort_index()\n",
    "        self.doc2vec_model = Doc2Vec.load(\"../data/en_wikipedia_corpus/doc2vec_eng.pickel\")\n",
    "\n",
    "\n",
    "    def _compose_document(self, element):\n",
    "        # example element:\n",
    "        # {'leisure pitch': {'area': 373.27528143301606}, 'sport basketball': {'area': 373.27528143301606}}\n",
    "        doc_shard = ''\n",
    "        for key in element.keys():\n",
    "            group, sub_type =  key.split(' ')\n",
    "            # we lookup the combination in the tag_df\n",
    "            row = self.tag_df.loc[group, sub_type]\n",
    "            # and retrieve both the wikidata and the wikipedia text from it\n",
    "            wikidata_desc = row.iloc[0].wikidata_desc\n",
    "            wikipedia_desc = row.iloc[0].en_text\n",
    "\n",
    "            # now we test if we aktualy have wikipedia text\n",
    "            if wikipedia_desc:\n",
    "                text = wikipedia_desc\n",
    "            # if not we take the wikidata description instead\n",
    "            else:\n",
    "                text = wikidata_desc\n",
    "\n",
    "            # now we test if we have an area or a lenght for the group sub_type combo\n",
    "            if element[key]:\n",
    "                # this is uqly and needs restructuring:\n",
    "                if element[key].get('area'):\n",
    "                    area = element[key].get('area')\n",
    "                    # now we need to get the median area for this group sub_type combo\n",
    "                    try:\n",
    "                        m_area = self.tag_median_size_df.loc[group,sub_type].median_area\n",
    "                    except:\n",
    "                        m_area = None\n",
    "\n",
    "                    if m_area:\n",
    "                        doc_shard += self._text_weigher(text,area/m_area)\n",
    "                    else:\n",
    "                        # if we dont have an median we wiegh with 0\n",
    "                        doc_shard += self._text_weigher(text,0)  \n",
    "\n",
    "                if element[key].get('length'):\n",
    "                    length = element[key].get('length')\n",
    "                    # now we need to get the median area for this group sub_type combo\n",
    "                    try:\n",
    "                        m_length = self.tag_median_size_df.loc[group,sub_type].median_length\n",
    "                    except:\n",
    "                        m_length = None\n",
    "\n",
    "                    if m_length:\n",
    "                        doc_shard += self._text_weigher(text,length/m_length)\n",
    "                    else:\n",
    "                        # if we dont have an median we wiegh with 0\n",
    "                        doc_shard += self._text_weigher(text,0)                \n",
    "\n",
    "            else:\n",
    "                # else is the case when the element is just a node thus it does not\n",
    "                # have an area or length\n",
    "                doc_shard += self._text_weigher(text,0)\n",
    "\n",
    "        return doc_shard\n",
    "\n",
    "    def _text_weigher(self, text, weight):\n",
    "        # for the time beeing we just round the weight to the next intiger\n",
    "        weight = round(weight)\n",
    "\n",
    "        if weight <= 1 :\n",
    "            # we want the text atleast once \n",
    "            w_text = text\n",
    "\n",
    "        else:\n",
    "            text += ' '\n",
    "            w_text = text*weight\n",
    "\n",
    "        return w_text\n",
    "\n",
    "    def _clip_2d_features(self, feature):\n",
    "        feature_cliped = {}\n",
    "        for geometry in ['line','polygon', 'multipolygon', 'multiline', 'multipoint']:\n",
    "            if feature.get(geometry):\n",
    "                # if we found a geometry we clip it to the extent of the bounding box\n",
    "                intersection = self.boundingbox.intersection(feature.get(geometry).buffer(0))\n",
    "                if intersection:\n",
    "                    feature_cliped[geometry] = intersection\n",
    "        if 'point' in feature:\n",
    "            # points are simply copied over\n",
    "            feature_cliped['point'] = feature['point']\n",
    "\n",
    "        return feature_cliped\n",
    "\n",
    "    def generate_vec(self, boundingbox):\n",
    "        self.boundingbox = boundingbox\n",
    "        # first we fetch for the given boundingbox features from OSM:\n",
    "        osm_handle = parse_osm.disect_osm(parse_osm.json_from_osm(boundingbox))\n",
    "        # from these features we filter out the features with tags that we have documents for \n",
    "        selected_tags_df = osm_handle.feature_df[osm_handle.feature_df.apply(self.filtertags_handler.filter_them, axis = 1)]\n",
    "        # we then cast these features to a list so that in the next step we generate thier geometry\n",
    "        type_id_list = selected_tags_df.index.tolist()\n",
    "        # for the selected features we calculate the geometry:\n",
    "        [osm_handle.get_geometry(f_type,osm_id) for f_type, osm_id in type_id_list]\n",
    "        #then we have to reslect the dataframe and make a copy of it for further proccesing\n",
    "        selected_tags_df = osm_handle.feature_df[osm_handle.feature_df.apply(self.filtertags_handler.filter_them, axis = 1)].copy()\n",
    "        # we need to clip the 2 dimensional feautres to the extent of the $boundingbox\n",
    "        selected_tags_df['geometry'] = selected_tags_df['geometry'].apply(self._clip_2d_features)\n",
    "        # filter out feauters who no longer have a geometry after clipping\n",
    "        selected_tags_df = selected_tags_df.loc[selected_tags_df['geometry']!={}]\n",
    "        # lastly for the found featres with tags that intresst us we wiegh thier size \n",
    "        # (if ther is one) and combine them into a meta document\n",
    "        combined_document = ''\n",
    "        for element in selected_tags_df[['geometry','tags']].apply(self.filtertags_handler.calcualte_size_for_tags, axis=1).tolist():\n",
    "            # and then we add the document shard for a given element\n",
    "            combined_document += self._compose_document(element)\n",
    "            # some padding\n",
    "            combined_document += ' '\n",
    "        # the combinded document we feed into ou doc2vec model and generate a vector\n",
    "        vec = self.doc2vec_model.infer_vector(combined_document.split(' '))\n",
    "\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mineral-lebanon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.24275461e-01, -1.30907357e-01, -6.22762716e-04, -5.50206602e-01,\n",
       "        1.39365107e-01, -3.68225366e-01,  5.71599901e-01,  1.99213296e-01,\n",
       "        3.69874805e-01,  6.50141761e-02,  3.07978004e-01, -2.23174728e-02,\n",
       "        2.33467370e-01, -4.96434838e-01,  3.07873845e-01,  1.13946892e-01,\n",
       "       -1.01813659e-01,  2.72812188e-01, -5.24157226e-01, -5.86416006e-01,\n",
       "        2.76123941e-01,  4.62491930e-01,  4.43891555e-01, -1.99367613e-01,\n",
       "        3.48979294e-01,  7.27404933e-03,  3.55110914e-01,  3.96709144e-02,\n",
       "       -2.61587173e-01,  3.71331833e-02,  1.22273289e-01, -7.08876550e-02,\n",
       "        3.53142351e-01, -2.90880948e-02, -2.13600039e-01, -6.20221347e-02,\n",
       "       -4.36727703e-02,  5.20475924e-01, -2.60960221e-01,  3.42150852e-02,\n",
       "       -2.39130139e-01,  2.94416785e-01, -4.03456837e-01, -9.16286930e-02,\n",
       "       -2.48920530e-01, -2.21156955e-01, -1.37600034e-01, -2.51091868e-01,\n",
       "        8.15836936e-02,  7.24368542e-03,  3.92527990e-02, -3.04578602e-01,\n",
       "        5.76640032e-02, -1.70597643e-01, -2.42910266e-01, -2.64365762e-01,\n",
       "       -3.27046573e-01,  4.36872303e-01, -2.90155232e-01,  4.48692352e-01,\n",
       "       -1.24568850e-01,  3.33937526e-01, -6.41462654e-02,  3.63309264e-01,\n",
       "        5.84005535e-01, -1.45377174e-01, -2.49582514e-01,  5.65139651e-01,\n",
       "       -4.38407734e-02,  1.06812790e-01,  2.05622017e-01, -3.12627405e-01,\n",
       "        1.62510946e-01,  1.28322184e-01, -7.09498301e-02,  3.45610827e-01,\n",
       "       -3.82864624e-01,  1.01507619e-01,  2.31965959e-01,  2.38415450e-01,\n",
       "        1.65241539e-01, -1.56531528e-01, -4.65404421e-01, -3.53483260e-01,\n",
       "        2.05428585e-01,  3.83050025e-01, -9.40234661e-02, -1.59295112e-01,\n",
       "       -3.89207602e-01,  2.30837449e-01,  1.88600734e-01,  1.07467167e-01,\n",
       "       -1.86225459e-01, -2.48012282e-02,  2.23067299e-01,  1.66464880e-01,\n",
       "        3.92523184e-02, -2.02031471e-02,  1.40912876e-01, -2.76106179e-01,\n",
       "       -2.42007464e-01, -7.47638792e-02, -3.21963489e-01, -3.44615042e-01,\n",
       "       -2.27609351e-01, -1.80025265e-01,  3.51709962e-01, -4.40345332e-02,\n",
       "       -2.44771495e-01,  6.10924721e-01,  2.30203509e-01,  2.81946033e-01,\n",
       "       -9.03512463e-02,  2.72703588e-01, -1.03579469e-01, -2.04389706e-01,\n",
       "       -3.73599976e-02,  2.71627992e-01, -2.43067190e-01, -1.48102835e-01,\n",
       "       -3.65950495e-01, -1.47300372e-02,  2.80924857e-01, -8.17437410e-01,\n",
       "        1.07331045e-01,  3.45788062e-01,  1.00735106e-01,  5.08007128e-03,\n",
       "        2.37805620e-01,  1.42540589e-01,  2.68513203e-01,  1.44509241e-01,\n",
       "       -3.23530704e-01,  2.10047767e-01, -2.33928964e-01,  6.07135519e-02,\n",
       "        1.97865143e-01, -2.76347160e-01, -6.21074557e-01, -8.62180144e-02,\n",
       "       -3.03887933e-01,  2.88136750e-01,  2.98017353e-01, -4.16149437e-01,\n",
       "        2.41835658e-02,  9.43739936e-02, -9.42083746e-02, -4.91486222e-01,\n",
       "        1.91826776e-01, -2.76829712e-02, -2.50331610e-02,  4.57008541e-01,\n",
       "       -3.29145789e-02, -1.00814648e-01, -5.05551100e-01,  4.07619290e-02,\n",
       "        8.94601941e-02,  6.79674372e-02,  2.28384227e-01, -1.35304421e-01,\n",
       "        2.94256993e-02, -5.58243096e-01,  4.13319170e-02,  1.07527852e-01,\n",
       "        3.02610993e-01, -3.74745965e-01, -1.55536398e-01,  3.89274687e-01,\n",
       "       -1.97939664e-01, -2.11663678e-01,  1.44292131e-01,  1.17587984e-01,\n",
       "        5.45890182e-02, -3.24047148e-01,  2.01774791e-01,  4.99452427e-02,\n",
       "        1.50077119e-01, -1.89796194e-01, -4.22661230e-02, -1.22765722e-02,\n",
       "        1.70783579e-01, -7.69126117e-01, -2.97200331e-03,  1.52333349e-01,\n",
       "        1.05113804e-01, -2.78686285e-01,  7.48888124e-03,  2.56629754e-02,\n",
       "        2.84019709e-02,  2.09608078e-01,  3.30906622e-02, -2.78305769e-01,\n",
       "       -9.85597968e-02, -2.58916706e-01, -4.95822132e-02,  7.70991564e-01,\n",
       "        3.37914713e-02, -3.21049511e-01, -3.45336616e-01,  6.35782838e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_semantic_vector_obj = Spatial_Semantic_Vector('../data/spatial_semantics/kv_df_just_eng.pickle','../data/spatial_semantics/tag_sizes_median_df.pickle')\n",
    "q_poly = Polygon([(16.36896371841431,48.20063653233946),(16.373598575592045,48.19960677385028),(16.371034383773807,48.19771882952509),(16.36772990226746,48.19816221664037)])\n",
    "spatial_semantic_vector_obj.generate_vec(q_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df =  pd.read_pickle('../data/spatial_semantics/kv_df_just_eng.pickle')\n",
    "tag_median_size_df = pd.read_pickle('../data/spatial_semantics/tag_sizes_median_df.pickle').set_index(['key','value']).sort_index()\n",
    "#replace pd nan with None\n",
    "tag_median_size_df = tag_median_size_df.where(pd.notnull(tag_median_size_df), None)\n",
    "filtertags_handler = tag_handler.filtertags(tag_df)\n",
    "# from here on out we want key value in the tag_df as the index because it makes\n",
    "# the look up faster\n",
    "# we sort the index to handle the \"1: PerformanceWarning: indexing past lexsort \n",
    "# depth may impact performance.\" Warning\n",
    "tag_df = tag_df.set_index(['key','value']).sort_index()\n",
    "# load doc2vec model\n",
    "doc2vec_model = Doc2Vec.load(\"../data/en_wikipedia_corpus/doc2vec_eng.pickel\")\n",
    "\n",
    "# TU Wien polygon\n",
    "q_poly = Polygon([(16.36896371841431,48.20063653233946),(16.373598575592045,48.19960677385028),(16.371034383773807,48.19771882952509),(16.36772990226746,48.19816221664037)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_handle = parse_osm.disect_osm(parse_osm.json_from_osm(q_poly))\n",
    "osm_handle.feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tags_df = osm_handle.feature_df[osm_handle.feature_df.apply(filtertags_handler.filter_them, axis = 1)]\n",
    "type_id_list = selected_tags_df.index.tolist()\n",
    "# for the selected features we calculate the geometry:\n",
    "[osm_handle.get_geometry(f_type,osm_id) for f_type, osm_id in type_id_list]\n",
    "# reselect to fetch the generated geometries\n",
    "selected_tags_df = osm_handle.feature_df[osm_handle.feature_df.apply(filtertags_handler.filter_them, axis = 1)].copy()\n",
    "selected_tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to clip the 2 dimensional feautres \n",
    "selected_tags_df['geometry'] = selected_tags_df['geometry'].apply(clip_2d_features)\n",
    "# filter out feauters who no longer have a geometry after clipping\n",
    "selected_tags_df = selected_tags_df.loc[selected_tags_df['geometry']!={}]\n",
    "selected_tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_document = ''\n",
    "for element in selected_tags_df[['geometry','tags']].apply(filtertags_handler.calcualte_size_for_tags, axis=1).tolist():\n",
    "    # and then we add the document shard for a given element\n",
    "    combined_document += compose_document(element)\n",
    "    # some padding\n",
    "    combined_document += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyleaflet\n",
    "from ipyleaflet import Map, basemaps, basemap_to_tiles, GeoJSON, WKTLayer\n",
    "from ipywidgets import Label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "m = Map(\n",
    "    basemap=basemaps.CartoDB.Positron,\n",
    "    # for some reason lat lon are switch for centering the map\n",
    "    center=((q_poly.centroid.coords[0][1],q_poly.centroid.coords[0][0])),\n",
    "    #center=((test_multi['multipolygon'].centroid.coords[0][1],test_multi['multipolygon'].centroid.coords[0][0])),\n",
    "    zoom=14\n",
    ")\n",
    "\n",
    "wlayer = WKTLayer(\n",
    "    wkt_string=q_poly.wkt,\n",
    "    #hover_style={\"fillColor\": \"red\"},\n",
    "    fill_color=\"red\",\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "wlayer2 = WKTLayer(\n",
    "    wkt_string=selected_tags_df_test.loc['relation',22494].geometry_clipped['multiline'].wkt,\n",
    "    #hover_style={\"fillColor\": \"red\"},\n",
    "    fill_color=\"blue\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "m.add_layer(wlayer)\n",
    "m.add_layer(wlayer2)\n",
    "\n",
    "\n",
    "# m.add_layer(geo_json)\n",
    "\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
