{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generated_spatial_features_dataframe(path,limit=999999):\n",
    "    key_value_link_list = []\n",
    "    with open(path,'r') as f:\n",
    "        urls = f.read()\n",
    "        urls = urls.split('\\n')\n",
    "    \n",
    "    for url in urls[:limit]:\n",
    "        key_value_link_list = key_value_link_list + fetch_group(url)\n",
    "    \n",
    "    return pd.DataFrame(key_value_link_list,columns=['key','value','osm_wiki_url'])\n",
    "    \n",
    "\n",
    "def make_soup(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def fetch_group(url):\n",
    "    soup = make_soup(url)\n",
    "    tmp_list = []\n",
    "\n",
    "    for tbody in soup.find_all('tbody'):\n",
    "        for tr in tbody.find_all('tr'):\n",
    "            for td in tr.find_all('td'):\n",
    "                found_key = None\n",
    "                try:\n",
    "                    title = td.a['title']\n",
    "                    if 'Tag:' in title:\n",
    "                        key,value = title[4:].split('=')\n",
    "                        tmp_list.append((key,value,f\"https://wiki.openstreetmap.org{td.a['href']}\"))\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return tmp_list\n",
    "\n",
    "def get_wikidata(url):\n",
    "    wikidata_url = None\n",
    "    \n",
    "    try:\n",
    "        soup = make_soup(url)\n",
    "        wikidata_url = soup.find('tr', {'class' : 'd_wikidata content'}).a['href']\n",
    "    \n",
    "    except Exception as e:\n",
    "        wikidata_url = e\n",
    "    \n",
    "    return wikidata_url\n",
    "\n",
    "def get_wikipedia_urls(url):\n",
    "    \n",
    "    temp_dict = {}\n",
    "    soup = make_soup(url)\n",
    "    wikibase = soup.find(\"div\",{'class','wikibase-sitelinklistview'})\n",
    "    \n",
    "    # if the page does not contain a class=wikibase-sitelinklistview we ignore it\n",
    "    if wikibase:\n",
    "        for li in wikibase.find_all('li'):\n",
    "            link = li.find('a')['href']\n",
    "            lang = li.findAll('span')[3]['lang']\n",
    "            temp_dict[lang]=link\n",
    "    else:\n",
    "        temp_dict = None\n",
    "    \n",
    "    return temp_dict\n",
    "\n",
    "def scrub_text(text):\n",
    "    # Drop footnote superscripts in brackets\n",
    "    text = re.sub(r'\\[.*?\\]+', '', text)\n",
    "    # Replace all non word charcters with a white space\n",
    "    text = re.sub('\\W', ' ',text)\n",
    "    # trim all white spaces >2 to 1 \n",
    "    text = re.sub('\\s{2,}',' ',text)\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def plain_text_from_wiki(url):\n",
    "    soup = make_soup(url)\n",
    "    text = ''\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        text += paragraph.text\n",
    "    plain_text = scrub_text(text)\n",
    "    return plain_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_df = generated_spatial_features_dataframe('./osm_groups.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_df['wikidata_url'] = kv_df['osm_wiki_url'].apply(get_wikidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some wikidata_urls contain an except these need to be filtered out\n",
    "kv_df = kv_df.loc[kv_df['wikidata_url'].astype(str).str.contains('http')]\n",
    "kv_df['wikipedia_dict'] = kv_df['wikidata_url'].apply(get_wikipedia_urls)\n",
    "kv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we just select the english 'en' wikipedia entries and drop those rows that don't have an english wikipedia entry\n",
    "kv_df_just_eng = pd.concat([kv_df.drop(['wikipedia_dict'], axis=1), kv_df['wikipedia_dict'].apply(pd.Series)], axis=1)[['key','value','en']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly we fetch the text\n",
    "kv_df_just_eng['en_text'] = kv_df_just_eng['en'].apply(plain_text_from_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and save the dataframe\n",
    "kv_df_just_eng.to_pickle('./kv_df_just_eng.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_df_just_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(small_df.loc[3]['wikidata_url'])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "soup.findAll(\"div\", {\"class\": \"stylelistrow\"})\n",
    "wikibase = soup.find(\"div\",{'class','wikibase-sitelinklistview'})\n",
    "for li in wikibase.find_all('li'):\n",
    "    print(li.findAll('span')[3]['lang'])\n",
    "#     link = li.find('a')['href']\n",
    "#     lang = li.findAll('span')[1]['title']\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
